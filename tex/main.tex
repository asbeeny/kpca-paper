\documentclass{article}

%%% Begin imports
% 
% math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
% 
% theorems
\usepackage{amsthm}
% 
% algorithm / code listing
\usepackage[boxruled]{algorithm2e}
\usepackage{listings}
% 
% graphics
\usepackage{float}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
% 
% links and cross-referencing
% these have to be loaded as late as possible, cleveref last
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\crefname{enumi}{part}{parts}
\Crefname{enumi}{Part}{Parts}
%
%%% End imports

%%% Begin environments
% these have to be loaded after cleveref
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
%%% End environments

%%% Begin macros
% symbols
\def\FF{\mathbb{F}}
\def\RR{\mathbb{R}}
\def\CC{\mathbb{C}}
\def\NN{\mathbb{N}}
\def\QQ{\mathbb{Q}}
\def\hilbert#1{\mathcal{#1}} % define the look of Hilbert spaces
\def\X{\hilbert{X}}
\def\H{\hilbert{H}}
\def\dotprod{\boldsymbol{\cdot}}
\def\MSE{\mathrm{MSE}}

% listing style
\lstset{
    basicstyle=\ttfamily\small,
    commentstyle=\itshape\color{gray},
    numbers=left,
    numberstyle=\tiny
}

% commands
\def\ipt#1{\left\langle #1 \right\rangle}

% operators
\DeclareMathOperator{\lspan}{span}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\rowmean}{row\,mean}
\DeclareMathOperator{\colmean}{col\,mean}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\Var}{var} 
\DeclareMathOperator{\tr}{tr} 
%%% End macros

%%% Set title
\title{Kernel Principal Component Analysis}
\author{Alex Beeny \url{abeeny@siue.edu}}
\date{Spring 2024}

\setcounter{tocdepth}{1}

\begin{document}
\maketitle
\tableofcontents
\begin{abstract}
    Principal component analysis (PCA) and kernel methods are tools often used in data science.
    The underlying theory of these tools depend on the properties of a special type of Hilbert space called a reproducing kernel Hilbert space (RKHS).
    This paper explores the essence of RKHSs using data science examples, in particular, PCA and kernel PCA.
    When kernel methods are applied to PCA, we can analyze nonlinear data in a high-dimensional feature space with some nice properties.
\end{abstract}
\section{Introduction}
\label{sec:introduction}
\input{sections/010-introduction.tex}
\section{Principal Component Analysis}
\label{sec:principal-component-analysis}
\input{sections/020-pca.tex}
\section{Reproducing Kernel Hilbert Space}
\label{sec:reproducing-kernel-hilbert-space}
\input{sections/030-rkhs.tex}
\section{Kernel PCA}
\label{sec:kernel-pca}
\input{sections/040-kernel-pca.tex}
\section{Conclusion}
\label{sec:conclusion}
\input{sections/060-conclusion.tex}
\appendix
\section{Linear Algebra}
\label{sec:linear-algebra}
\input{sections/070-linear-algebra.tex}
% \section{Riesz Representation Theorem}
% \label{sec:riesz-representation-theorem}
% \input{sections/080-riesz-representation-theorem.tex}
% \section{Mercer's Theorem}
% \label{sec:mercers-theorem}
% \input{sections/090-mercers-theorem.tex}
\section{Code}
\label{sec:code}
\input{sections/100-code.tex}

% \nocite{*}
\bibliographystyle{plain}
\bibliography{refs}
\end{document}