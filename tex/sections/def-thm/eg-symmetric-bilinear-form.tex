If \(U = \{u_1, u_2, \dots, u_n\}\) is a basis for \(\X\), then we can define a matrix \(K = \begin{bmatrix}
    k(u_i, u_j)
\end{bmatrix}_{ij}\).
Clearly, \(K\) is symmetric since \(k(u_i, u_j) = k(u_j, u_i)\).
Let \(v = \sum_{i=1}^n \alpha_i u_i\) and \(w = \sum_{i=1}^n \beta_i u_i\) be vectors with respect to \(U\) and let \(x = [\alpha_i]_{i=1}^n\) and \(y = [\beta_i]_{i=1}^n\).
Then
\begin{equation}
    \label{eqn:bilinear-form-representation}
    k(v,w)
    = k\left(\sum_{i=1}^n \alpha_i u_i, \sum_{j=1}^{n} \beta_j u_j\right)
    = \sum_{i=1}^n \sum_{j=1}^{n} \alpha_i \beta_j k\left(u_i, u_j\right)
    = x^\top K y.
\end{equation}
If \(K = I\), then \(v = x\), \(w = y\), and \(k(v,w) = v^\top w\) is simply the dot product.
Otherwise, if \(K\) is positive semidefinite, then \(K = B^\top B\) implies
\begin{equation}
    \label{eqn:bilinear-form-dot-product}
    k(v,w) = x^\top B^\top B y = (Bx)^\top (By)
\end{equation}
In this case, \(k(v,w)\) is just the dot product after the transformation under \(B\).
Notice that if \(U\) is merely a subset of \(\X\), then \(v\) and \(w\) no longer have unique representations, but \cref{eqn:bilinear-form-representation,eqn:bilinear-form-dot-product} are still valid for all \(v, w \in \lspan U\).

We say that \(k\) is \textit{positive semidefinite} if \(K = [k(u_i, u_j)]_{ij}\) is a positive semidefinite matrix for any finite subset \(U = \{u_1, u_2,\dots, u_n\} \subseteq \X\).
Then \(K\) is a Gram matrix with respect to some set of transformed vectors related to \(U\) and some inner product related to \(k\).
In the next subsection, we will show that \(k\) still corresponds to some inner product even if \(k\) is not bilinear.
