Recall that the PCA algorithm involves minimizing terms involving the dot product \(x^\top x\).
We would like to apply a feature map to our variables and replace this dot product with an inner product in the feature space.
However, since the dimension of the feature space will typically be much larger than the dimension of the original space, optimization using this inner product will be expensive.
This problem is solved by the so-called \textit{kernel trick} which allows us to circumvent the computational complexity issue caused by high-dimensional spaces.
In order to justify the kernel trick, we need to establish some theory of reproducing kernel Hilbert spaces.

First, we recall the definition of an inner product.
\begin{definition}
    \cite[p. 10]{small1994hilbert}
    Let \(X\) be a (real) vector space.
    An \textbf{inner product} is a function \(\langle \cdot, \cdot \rangle : X \times X \to \RR\) which satisfies the following properties:
    \begin{enumerate}
        \item Symmetry. For all \(x,y \in X\),
        \[\langle x,y \rangle = \langle y, x \rangle.\]
        \item Linear in the first argument. For all \(x,y,z \in X\), \(\alpha, \beta \in \RR\),
        \[\langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle.\]
        \item Positive definite. For all \(x \in X\),
        \[\langle x, x \rangle \geq 0\]
        and \(\langle x, x \rangle = 0\) if and only if \(x = 0\).
    \end{enumerate}
    An inner product space is a vector space along with an inner product.
\end{definition}

Since real inner products are symmetric and linear in the first argument,
\[
    \langle z, \alpha x + \beta y\rangle
    = \langle \alpha x + \beta y, z \rangle
    = \alpha \langle x, z \rangle + \beta \langle y, z \rangle
    = \alpha \langle z, x \rangle + \beta \langle z, y \rangle
.\]
So, we also have linearity in the second argument.

The norm induced by an inner product is defined as
\[\|x\| = \langle x, x \rangle^{1/2}\]
and the metric induced by this norm is
\[d(x,y) = \|x - y\| = \langle x-y, x-y \rangle^{1/2}.\]
It follows that an inner product space is also a normed space and a metric space.
So, the induced norm will have the following properties for all \(x,y \in X\) and \(\alpha \in \RR\):
\begin{enumerate}
    \item Triangle inequality. \(\|x + y \| \leq \|x\| + \|y\|\);
    \item Scalar multiplication. \(\|\alpha x\| = |\alpha| \|x\|\);
    \item Positivity. \(\|x\| \geq 0\) and \(\|x\| = 0\) if and only if \(x = 0\). 
\end{enumerate}

\begin{definition}[Hilbert space]
    A Hilbert space is a complete inner product space.
    Let \(H\) be a Hilbert space.
    Then we denote the inner product as \(\langle \cdot, \cdot \rangle_H\).
\end{definition}

\begin{definition}[kernel]
    Let \(X\) be a nonempty set and \(k : X \times X \to \RR\).
    Then \(k\) is a (positive semi-definite) \textbf{kernel} if
    \begin{enumerate}
        \item \(k\) is symmetric: for all \(x,y \in X\),
        \[k(x,y) = k(y,x);\]
        \item \(k\) is positive semi-definite: for all \(n \in \NN\), if \(x_1, \dots, x_n \in X\) and \(c_1, \dots, c_n \in \RR\), then
        \[\sum_{i=1}^{n}\sum_{j=1}^{n} c_i c_j k(x_i, x_j) \geq 0.\]
        Equivalently, the matrix \(K \in \RR^{n \times n}\) whose entries are \(K_{ij} = k(x_i, x_j)\) is positive semi-definite, that is, \(w^\top K w \geq 0\) for all \(w \in \RR^n\).
    \end{enumerate}
\end{definition}

\begin{definition}[feature map]
    \cite{rudin2023notes}
    Let \(H\) be a Hilbert space of functions \(f : X \to \RR\).
    A feature map is a function \(\Phi : X \to H\).
\end{definition}

Our goal is to have kernels written as inner products of feature maps:
\[k(x,y) = \langle \Phi(x), \Phi(y) \rangle_H.\]
If \(\Phi\) is a known feature map, then \(k\) would surely be symmetric and positive definite since inner products are.
If instead we know \(k\) is a kernel, then we need to show that there is a unique Hilbert space with the desired inner product.

\subsubsection*{To do:}
\begin{itemize}
    \item move kernel/Gram matrix to its own definition
    \item Define linear functionals
    \item Define dual space (maybe?)
    \item Riesz representation theorem
    \item Define evaluation functionals
    \item Define RKHS (continuous evaluation functionals)
    \item An RKHS defines a unique reproducing kernel (by Riesz representation theorem).
    \item Mercer's theorem.
    \item A kernel defines a feature map.
    \item A kernel defines a unique RKHS (by Mercer/Moore-Aronszajn).
    \item We finally get \(k(x,y) = \langle \Phi(x), \Phi(y) \rangle_H\).
\end{itemize}

\begin{theorem}
    \cite{rudin2023notes,shawe2004kernel}
    Suppose \(k_1\) and \(k_2\) are kernels over \(X \times X\).
    The following functions kernels.
    \begin{enumerate}
        \item \(k(x,y) = a_1 k_1(x,y) + a_2 k_2(x,y)\) for all \(a_1, a_2 \geq 0\).
        \item \(k(x,y) = k_1(x,y) k_2(x,y)\).
        \item \(k(x,y) = k_1(h(x),h(y))\) for all \(h : X \to X\).
        \item \(k(x,y) = g(x)g(y)\) for all \(g : X \to \RR\).
        \item \(k(x,y) = a_0 + a_1 k_1(x,y) + a_2 k_1(x,y)^2 + \cdots + a_n k_1(x,y)^n\) for all \(n \in \NN\) and \(a_0, \dots, a_n \geq 0\).
        \item \(k(x,y) = \exp(k_1(x,y))\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Let \(x_1, \dots, x_n \in X\).
    \begin{enumerate}
        \item Let \(a_1, a_2 \geq 0\).
        Define the matrices \(K_1\) and \(K_2\) by
        \begin{align*}
            [K_1]_{ij} &= k_1(x_i,x_j),&
            [K_2]_{ij} &= k_2(x_i,x_j),
        \end{align*}
        for \(i,j = 1,\dots,n\).
        Let \(K = a_1 K_1 + a_2 K_2\).
        Since \(K_1\) and \(K_2\) are symmetric, so is \(K\).
        Since \(K_1\) and \(K_2\) are positive semi-definite and \(a_1, a_2 \geq 0\),
        \[w^\top K w = w^\top (a_1 K_1 + a_2 K_2) w = a_1 (w^\top K_1 w) + a_2 (w^\top K_2 w) \geq 0,\]
        for any \(w \in \RR^n\).
        It follows that \(K\) is symmetric positive semi-definite.
        Since \(K\) is the Gram matrix for \(k\), \(k\) is a kernel.
        \item 
        \item 
        \item 
        \item 
        \item 
    \end{enumerate}
\end{proof}

\begin{theorem}[Gaussian kernel]
    The function \(k : \RR^n \times \RR^n \to \RR\) defined by
    \[k(x,y) = \exp\left(\dfrac{-\|x-y\|^2_2}{\sigma^2}\right),\]
    is a kernel.
\end{theorem}