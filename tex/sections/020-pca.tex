
\input{sections/intro-pca.tex}

\subsection{Finding uncorrelated features}
\label{sub:finding-uncorrelated-features}
\input{sections/sub-uncorrelated-features.tex}

\subsection{Singular value decomposition}
\label{sub:singular-value-decomposition}
\input{sections/sub-singular-value-decomposition.tex}

\subsection{Minimizing projection residuals}
\label{sub:minimizing-projection-residuals}
\input{sections/sub-minimize-projection-residuals.tex}

\subsection{PCA algorithm}
\input{sections/sub-pca-algo.tex}

\begin{example}
    \label{eg:pca}
    \input{sections/def-thm/eg-pca.tex}
\end{example}

\subsection{Applications of PCA}

One of the most common applications of PCA is \textit{dimension reduction}.
When variables are correlated, the observations lie in some linear subspace of the original space.
In this situation, PCA can be used to project down to the lower dimension and have the smallest possible projection error.
This is particularly useful when the dimension of the input space is extremely large.
A number challenges arise when analyzing high-dimensional data and are collectively referred to as the \textit{curse of dimensionality} \cite{koutroumbas2008pattern}.
By working in the PCA feature space, these problems may be avoided.

\begin{example}
    \label{eg:pca-code}
    \input{sections/def-thm/eg-pca-code.tex}
\end{example}

% \subsection{Linear regression and PCA}
% \input{sections/sub-linear-regression-and-pca.tex}

% \begin{example}
%     \input{sections/def-thm/eg-linear-regression-and-pca.tex}
% \end{example}