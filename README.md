# Kernel PCA Project

Alex Beeny

Current project paper is here: ([tex/main.pdf](tex/main.pdf)).

## Desmos links
- [confidence ellipse](https://www.desmos.com/calculator/qcwpzyp9np)
- [OLS vs TLS](https://www.desmos.com/calculator/apqmlrjee9)

## TODO:

- [ ] Cynthia Rudin proofs 1-7 [mit-notes](https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/c80897854282a1fb65c3c99488f6ae50_MIT15_097S12_lec13.pdf#page=18)

### 1 Introduction
- [ ] clean up example
- [ ] show the idea of feature maps
- [ ] more figures?
- [ ] talk about statistical learning theory?
- [ ] introduce dimension reduction

### 2 Principal Component Analysis
- [ ] sketch out main idea
- [ ] dot product and covariance
- [ ] show PCA minimizes projection residuals (Shawe-Taylor)
- [ ] maximizing variance
- [ ] more on dimension reduction (low priority)
- [ ] algorithm
- [ ] example(s)

### 3 Reproducing Kernel Hilbert Space
- [ ] inner product/space
- [ ] normed space, induced norm, Banach space
- [ ] kernel
- [ ] properties of kernels
- [ ] reproducing property
- [ ] Hilbert space
- [ ] linear functionals
- [ ] linear functionals are continuous iff bounded
- [ ] Hilbert space dual
- [ ] Riesz representation
- [ ] evaluation functional
- [ ] evaluation functionals in dual implies existence of reproducing kernel
- [ ] RKHS
- [ ] RKHS properties
- [ ] Mercer's Theorem

### 4 Kernel PCA
- [ ] kernel trick
- [ ] algorithm
- [ ] example
- [ ] Gaussian kernel

### 5 Conclusion
- [ ] applications
- [ ] integral operators
- [ ] harmonic/functional analysis

### Appendix
- [ ] linear algebra basics (low priority)
- [ ] Frobenius norm (Mohri)
- [ ] SVD (Mohri)
- [ ] Riesz Representation (Small)
- [ ] Mercer's Theorem (Rudin)
- [ ] code examples