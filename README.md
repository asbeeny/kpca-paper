# Kernel PCA Project

Alex Beeny

Current project paper is here: ([tex/main.pdf](tex/main.pdf)).

## Desmos links
- [confidence ellipse](https://www.desmos.com/calculator/qcwpzyp9np)
- [OLS vs TLS](https://www.desmos.com/calculator/apqmlrjee9)

## TODO:

### 1 Introduction
- [ ] clean up example
- [ ] show the idea of feature maps
- [ ] more figures?
- [ ] talk about statistical learning theory?
- [ ] introduce dimension reduction

### 2 Principal Component Analysis
- [ ] sketch out main idea
- [ ] dot product and covariance
- [ ] show PCA minimizes projection residuals (Shawe-Taylor)
- [ ] maximizing variance
- [ ] more on dimension reduction (low priority)
- [ ] algorithm
- [ ] example(s)

### 3 Reproducing Kernel Hilbert Space
- [ ] inner product/space
- [ ] normed space, induced norm, Banach space
- [ ] kernel
- [ ] properties of kernels
- [ ] reproducing property
- [ ] Hilbert space
- [ ] linear functionals
- [ ] linear functionals are continuous iff bounded
- [ ] Hilbert space dual
- [ ] Riesz representation
- [ ] evaluation functional
- [ ] evaluation functionals in dual implies existence of reproducing kernel
- [ ] RKHS
- [ ] RKHS properties
- [ ] Mercer's Theorem

### 4 Kernel PCA
- [ ] kernel trick
- [ ] algorithm
- [ ] example
- [ ] Gaussian kernel

### 5 Conclusion
- [ ] applications
- [ ] integral operators
- [ ] harmonic/functional analysis

### Appendix
- [ ] linear algebra basics (low priority)
- [ ] Frobenius norm (Mohri)
- [ ] SVD (Mohri)
- [ ] Riesz Representation (Small)
- [ ] Mercer's Theorem (Rudin)
- [ ] code examples